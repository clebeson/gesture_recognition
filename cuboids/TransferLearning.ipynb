{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento de Gesto Dinâmicos Utilizando Redes Neurais Profundas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>**Disciplina**: Introdução a redes neurais profundas 2017/1\n",
    "<br/>**Professor**: Jorge Leonid Aching Samatelo\n",
    "<br/>**Alunos**: Clebeson Canuto dos Santos e Leonardo de Assis Silva\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Códigos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_steps = 8e4\n",
    "\n",
    "initial_learning_rate = 1e-3\n",
    "decay_steps = 7e2\n",
    "decay_rate = 0.98\n",
    "\n",
    "summary_step = 1e2\n",
    "batch_size = 256\n",
    "\n",
    "checkpoint_dir=\"./checkpoints\"\n",
    "\n",
    "n_classes = 10\n",
    "rows, cols, channels = 100, 100, 3\n",
    "input_file = './train.tfrecords'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_decode_distort(queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized = reader.read(queue)\n",
    "    features = tf.parse_single_example(serialized, features={\n",
    "      'image_raw': tf.FixedLenFeature([rows*cols*channels], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "    \n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    image = tf.cast(image, tf.float32) \n",
    "    image = tf.reshape(image, [rows, cols, channels])\n",
    "    \n",
    "    #image = tf.image.resize_image_with_crop_or_pad(image, 96, 96)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    #image = tf.image.random_brightness(image, max_delta=63)\n",
    "    #image = tf.image.random_contrast(image, lower=0.2, upper=1.8)\n",
    "    \n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    return image, label\n",
    "\n",
    "def load_inputs(filename, batch_size):\n",
    "    with tf.name_scope('input'):\n",
    "        queue = tf.train.string_input_producer([filename])\n",
    "        \n",
    "        image, label = read_decode_distort(queue)\n",
    "        \n",
    "        image_batch, label_batch = tf.train.shuffle_batch(\n",
    "            [image, label], \n",
    "            batch_size = batch_size, \n",
    "            num_threads = 4, \n",
    "            capacity = 1000 + 3*batch_size, \n",
    "            min_after_dequeue = 1000)\n",
    "    \n",
    "    #tf.summary.image('images', image_batch)\n",
    "    label_batch = tf.one_hot(label_batch, n_classes)\n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    images, labels = load_inputs(input_file, batch_size)\n",
    "    #images = tf.image.grayscale_to_rgb(tf.expand_dims(source_images[:,:,:,0], 3))\n",
    "\n",
    "keep = tf.placeholder(tf.float32)\n",
    "\n",
    "global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step, \n",
    "                                           decay_steps, decay_rate, staircase=True)\n",
    "tf.summary.scalar(\"learning_rate\", learning_rate)\n",
    "\n",
    "tensor_name = \"pool3\" #\"conv3_3/Relu\"\n",
    "\n",
    "with open(\"vgg16-20160129.tfmodel\", mode='rb') as f:\n",
    "    content = f.read()\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(content)\n",
    "    \n",
    "    graph_def = tf.graph_util.extract_sub_graph(graph_def, [\"images\", tensor_name])\n",
    "    #graph_def = tf.graph_util.extract_sub_graph(graph_def, [\"images\", \"pool5\"])\n",
    "    tf.import_graph_def(graph_def, input_map={\"images\": images})\n",
    "    \n",
    "del content\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "net = graph.get_tensor_by_name(\"import/{0}:0\".format(tensor_name))\n",
    "#net = graph.get_tensor_by_name(\"import/pool5:0\")\n",
    "\n",
    "out_shape = net.get_shape().as_list()\n",
    "net = tf.stop_gradient(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "depth = 4096\n",
    "weights = {\n",
    "    \"Wfc1\": tf.Variable(tf.truncated_normal([np.prod(out_shape[1:]), depth]), name=\"fc1/weights\"), \n",
    "    \"Wfc2\": tf.Variable(tf.truncated_normal([depth, depth]), name=\"fc2/weights\"), \n",
    "    \"Wfc3\": tf.Variable(tf.truncated_normal([depth, n_classes]), name=\"fc3/weights\")\n",
    "} \n",
    "\n",
    "biases = {\n",
    "    \"Bfc1\": tf.Variable(tf.truncated_normal([depth]), name=\"fc1/biases\"),\n",
    "    \"Bfc2\": tf.Variable(tf.truncated_normal([depth]), name=\"fc2/biases\"),\n",
    "    \"Bfc3\": tf.Variable(tf.truncated_normal([n_classes]), name=\"fc3/biases\")\n",
    "}\n",
    "\n",
    "def inference(net, weigths, biases, keep):\n",
    "    net = tf.reshape(net, [-1, weights['Wfc1'].get_shape().as_list()[0]], name=\"fc1/reshape\")\n",
    "    net = tf.add(tf.matmul(net, weights['Wfc1']), biases['Bfc1'], name=\"fc1/matmul-add\")\n",
    "    net = tf.nn.relu(net, name=\"fc1/relu\")\n",
    "    net = tf.nn.dropout(net, keep, name=\"fc1/dropout\")\n",
    "\n",
    "    net = tf.add(tf.matmul(net, weights['Wfc2']), biases['Bfc2'], name=\"fc2/matmul-add\")\n",
    "    net = tf.nn.relu(net, name=\"fc2/relu\")\n",
    "    net = tf.nn.dropout(net, keep, name=\"fc2/dropout\")\n",
    "\n",
    "    net = tf.add(tf.matmul(net, weights['Wfc3']), biases['Bfc3'], name=\"fc3/matmul-add\")\n",
    "    return net\n",
    "\n",
    "with tf.name_scope('model'):\n",
    "    pred = inference(net, weights, biases, keep)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=labels))\n",
    "    tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "with tf.name_scope('sgd'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "with tf.name_scope('accuracy'):\n",
    "    acc = tf.equal(tf.argmax(pred, 1), tf.argmax(labels, 1))\n",
    "    acc = tf.reduce_mean(tf.cast(acc, tf.float32))\n",
    "    tf.summary.scalar(\"accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def formatter(d): \n",
    "    return 'loss: {0}, acc: {1}%'.format(d.values()[0], d.values()[1]*100)\n",
    "with tf.device('/cpu:0'):\n",
    "    hooks = [\n",
    "        tf.train.StopAtStepHook(num_steps=max_steps),\n",
    "        tf.train.LoggingTensorHook([loss, acc], every_n_secs=5, formatter=formatter),\n",
    "        tf.train.NanTensorHook(loss),\n",
    "    ]\n",
    "\n",
    "    sess = tf.train.MonitoredTrainingSession(checkpoint_dir=checkpoint_dir,\n",
    "                                           hooks=hooks,\n",
    "                                           save_checkpoint_secs=10*60,\n",
    "                                           save_summaries_steps=summary_step)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    try:\n",
    "        while not coord.should_stop() or sess.should_stop():\n",
    "#                 test_accuracy, step = sess.run([acc, global_step],  feed_dict={keep: 01.0})\n",
    "#                 print(\"Test Accuracy: %2f\" % test_accuracy , \"global_step: %d\" % int(step//1))\n",
    "#                 break\n",
    "                _, step = sess.run([optimizer, global_step], feed_dict={keep: 0.5})\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training...')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "\n",
    "    coord.join(threads)\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
